{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
   },
   "source": [
    "# Qwen3 From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d41ae66-3a75-4d31-ad30-08951031ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
   "metadata": {
    "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a50f2a5-f09e-4046-99c9-c2dc4802de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass()\n",
    "class Qwen3Config:\n",
    "    vocab_size=151936\n",
    "    hidden_size=1024\n",
    "    intermediate_size=3072\n",
    "    num_hidden_layers=28\n",
    "    num_attention_heads=16\n",
    "    num_key_value_heads=8\n",
    "    attention_bias=False\n",
    "    head_dim=128\n",
    "    hidden_act=\"silu\"\n",
    "    max_position_embeddings=40_960\n",
    "    rms_norm_eps=1e-6\n",
    "    tie_word_embeddings=False\n",
    "    rope_theta=10000.0\n",
    "    dtyp=torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4f13b-6173-4631-9ff0-0822099f437b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe14c4b-37b1-43fa-872b-b5e0e4ab19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, config: Qwen3Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, cu_lens: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids shape: [seq_len]\n",
    "        \n",
    "        # [seq_len, hidden_size]\n",
    "        x = self.embed_tokens(input_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, positions, cu_lens)\n",
    "\n",
    "        # shape not change\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # [seq_len, hidden_size]\n",
    "        return x\n",
    "\n",
    "class Qwen3ForCausalLM(nn.Module):\n",
    "    def __init__(self, config: Qwen3Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = Qwen3Model(config)\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, cu_lens: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # [seq_len, hidden_size]\n",
    "        x = self.model(input_ids, positions, cu_lens)\n",
    "\n",
    "        # [seq_len, vocab_size]\n",
    "        # extract the last token of each sequence\n",
    "        x = self.lm_head(x[cu_lens[1:]-1, :])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0469f-d06e-46cd-824c-226c840d72cb",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc158161-df8b-4bc0-a5bb-f3f6f25c80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Qwen3Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.self_attn = Qwen3Attention(config)\n",
    "\n",
    "        self.post_attention_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.mlp = Qwen3MLP(config)\n",
    "\n",
    "    def forward(self, x, positions: torch.Tensor, cu_lens: torch.Tensor):\n",
    "        shortcut = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x = self.self_attn(x, positions, cu_lens)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.post_attention_layernorm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41fa82-4f1c-47b5-b760-71a4b83ed8aa",
   "metadata": {},
   "source": [
    "### Qwen3Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab063b4-04cd-4604-b4de-94723fda27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Attention(nn.Module):\n",
    "    def __init__(self, config: Qwen3Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * config.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * config.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * config.head_dim, bias=config.attention_bias)\n",
    "\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads * config.head_dim, config.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        self.q_norm = nn.RMSNorm(config.head_dim, eps=config.rms_norm_eps)\n",
    "        self.k_norm = nn.RMSNorm(config.head_dim, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.scale = self.config.head_dim**-0.5\n",
    "\n",
    "        self.rotary_embedding = RotaryEmbedding(config.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, x, positions: torch.Tensor, cu_lens: torch.Tensor):\n",
    "        seqlen, _ = x.shape\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(seqlen, self.config.num_attention_heads, self.config.head_dim)\n",
    "        k = k.view(seqlen, self.config.num_key_value_heads, self.config.head_dim)\n",
    "        v = v.view(seqlen, self.config.num_key_value_heads, self.config.head_dim)\n",
    "\n",
    "        # [qhead, seqlen, head_dim]\n",
    "        q = q.transpose(0, 1)\n",
    "        # [kvhead, seqlen, head_dim]\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "\n",
    "        \n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        \n",
    "        q = self.rotary_embedding(q, positions)\n",
    "        k = self.rotary_embedding(k, positions)\n",
    "\n",
    "\n",
    "        group_size = self.config.num_attention_heads // self.config.num_key_value_heads\n",
    "        \n",
    "        \"\"\"\n",
    "        [1,2,3].repeat_interleave(2, dim=0) => [1, 1, 2, 2, 3, 3]\n",
    "        \"\"\"\n",
    "        # [kv_head * group_size, seqlen, head_dim]\n",
    "        k = k.repeat_interleave(group_size, dim=0)\n",
    "        v = v.repeat_interleave(group_size, dim=0)\n",
    "\n",
    "\n",
    "        o = []\n",
    "        for i in range(cu_lens.shape[0] - 1):\n",
    "            start = cu_lens[i].item()\n",
    "            end = cu_lens[i + 1].item()\n",
    "            seqlen_i = end - start\n",
    "\n",
    "            q_i = q[:,start:end,:]\n",
    "            k_i = k[:,start:end,:]\n",
    "            v_i = v[:,start:end,:]\n",
    "\n",
    "            scores = q_i @ k_i.transpose(-2, -1)\n",
    "\n",
    "            mask = torch.triu(torch.ones(seqlen_i, seqlen_i, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "            scores = scores.masked_fill(mask, -torch.inf)\n",
    "            scores = scores * self.scale\n",
    "            weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "            # [q_head, seqlen, head_dim]\n",
    "            o_i = weights @ v_i\n",
    "\n",
    "            # [seqlen, q_head, head_dim]\n",
    "            o_i = o_i.transpose(0, 1)\n",
    "            \n",
    "            o.append(o_i.flatten(1))\n",
    "        \n",
    "        o = torch.concat(o, dim=0)\n",
    "        \n",
    "        out = self.o_proj(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54387bbb-0c55-4c93-89c4-7eab804aedff",
   "metadata": {},
   "source": [
    "## Rotary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d9f34e-4746-4ec5-ae21-e8a8d56c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            max_position_embeddings: int,\n",
    "            rope_theta: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.rope_theta = rope_theta\n",
    "\n",
    "        # 1 / theta^(0, 2, 4, ..., dim-2) / dim\n",
    "        inv_freq = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
    "\n",
    "        # position: [max_position_embeddings]\n",
    "        position = torch.arange(max_position_embeddings, dtype=torch.float)\n",
    "\n",
    "        # freqs: [max_position_embeddings, dim/2]\n",
    "        freqs = torch.einsum(\"i,j->ij\", position, inv_freq)\n",
    "\n",
    "        # cos: [max_position_embeddings, dim/2]\n",
    "        self.register_buffer(\"cos_cached\", torch.cos(freqs))\n",
    "        # sin: [max_position_embeddings, dim/2]\n",
    "        self.register_buffer(\"sin_cached\", torch.sin(freqs))\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, positions: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [heads, seqlen, head_dim]\n",
    "        seqlen = x.size(1)\n",
    "    \n",
    "        cos = self.cos_cached[positions]\n",
    "        sin = self.sin_cached[positions]\n",
    "        cos = cos.unsqueeze(0)  # [1, seqlen, dim/2]\n",
    "        sin = sin.unsqueeze(0)  # [1, seqlen, dim/2]\n",
    "    \n",
    "        x1, x2 = torch.chunk(x.float(), 2, dim=-1)\n",
    "        x_rotated = torch.zeros_like(x)\n",
    "        x_rotated[..., 0::2] = x1 * cos - x2 * sin\n",
    "        x_rotated[..., 1::2] = x2 * cos + x1 * sin\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acece40f-d493-4dc0-a44a-2bd93da14af5",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397f0070-4b73-49c1-9938-6663b1a0dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3MLP(nn.Module):\n",
    "    def __init__(self, config: Qwen3Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.gate_proj(x)\n",
    "        x2 = self.up_proj(x)\n",
    "        x = self.act_fn(x1) * x2\n",
    "        x = self.down_proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfa940-3201-47a9-b958-644821a39e51",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1eee2b2-1b25-4a90-80ad-e13bba26fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = Qwen3ForCausalLM(Qwen3Config())\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429cba7-dc54-4ee8-bd60-34f99dc57f83",
   "metadata": {},
   "source": [
    "## Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2ce705-3381-4e5d-9af3-df0b6328c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from safetensors import safe_open\n",
    "import glob\n",
    "\n",
    "huggingface_model_dir = '~/huggingface/Qwen3-0.6B/'\n",
    "\n",
    "def load_weight(huggingface_model_dir, model):\n",
    "    params = dict(model.named_parameters())\n",
    "    \n",
    "    for file in glob.glob(os.path.join(path, \"*.safetensors\")):\n",
    "        with safe_open(file, \"pt\", \"cpu\") as f:\n",
    "            for name in f.keys():\n",
    "                weight = f.get_tensor(name)\n",
    "                assert name in params, f\"Parameter {name} not found in model\"\n",
    "                param = params[name]\n",
    "                param.data.copy_(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8642c4-46a8-403c-9f31-9072e361f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.expanduser(\"~/huggingface/Qwen3-0.6B/\")\n",
    "load_weight(path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f4035-fc37-4a69-9aa2-76ab5b8786d9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c626a7-6981-44fe-9604-0fc6f825f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "qwen3_tokenizer = tokenizers.Tokenizer.from_file(path + \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e711d4b2-5260-478b-8c1b-d3fc0cd36777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9707, 11, 1879]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen3_tokenizer.encode(\"Hello, world\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa6d348-e300-413f-a857-3d7a58e648cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen3_tokenizer.decode(qwen3_tokenizer.encode(\"Hello, world\").ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030fb6d-b0dc-486f-8ced-2c8be371f194",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74bed6e9-df35-497f-8323-3a256e27707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(prompt: str, enable_think: bool = False) -> str:\n",
    "    prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    if enable_think is False:\n",
    "        prompt += \"<think>\\n\\n</think>\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924456d2-213e-4b1d-b61d-b4c55d49c100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWhat is the meaning of life?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_chat_template(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9a864f-22de-4d69-966e-1f9780cec00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b54f9ff-1a15-462b-903a-2763bb622d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_step(model, requests: list[Request]):\n",
    "    tokens = []\n",
    "    positions = []\n",
    "    cu_lens = [0]\n",
    "\n",
    "    for req in requests:\n",
    "        tokens.extend(req.tokens)\n",
    "        positions.extend(range(len(req.tokens)))\n",
    "        cu_lens.append(cu_lens[-1] + len(req.tokens))\n",
    "\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "    positions = torch.tensor(positions, dtype=torch.long, device=device)\n",
    "    cu_lens = torch.tensor(cu_lens, dtype=torch.long, device=device)\n",
    "    \n",
    "    # [len(reqs), vocob_size]\n",
    "    logits = model(tokens, positions, cu_lens)\n",
    "    next_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return next_tokens.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f85f4812-a325-4f5d-8443-c026c2d12308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Qwen3ForCausalLM, tokenizer, prompts: list[str], enable_think=True, max_new_tokens=64):\n",
    "    requests = []\n",
    "    for prompt in prompts:\n",
    "        prompt = apply_chat_template(prompt, enable_think)\n",
    "        tokens = qwen3_tokenizer.encode(prompt).ids\n",
    "        req = Request(tokens)\n",
    "        requests.append(req)\n",
    "    \n",
    "    eos_token = tokenizer.encode(\"<|im_end|>\").ids[0]\n",
    "\n",
    "    new_tokens = 0;\n",
    "    while len(requests) and new_tokens < max_new_tokens:\n",
    "        new_tokens += 1\n",
    "\n",
    "        tokens = generate_one_step(model, requests)\n",
    "        for req, token in zip(requests, tokens):\n",
    "            req.tokens.append(token)\n",
    "        \n",
    "        if new_tokens % 10 == 0:\n",
    "            print(\"==================================================\")\n",
    "            for i, req in enumerate(requests):\n",
    "                print(f\"request {i}:\\n\", tokenizer.decode(req.tokens, skip_special_tokens=False))\n",
    "                        \n",
    "        # remove finished requests\n",
    "        requests = [req for req in requests if req.tokens[-1] != eos_token]\n",
    "        \n",
    "    return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b900efd-4ffa-4282-ad84-fd855aed2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to\n",
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the meaning of life. First, I need to consider\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to get started with LLMs. Let me break\n",
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the meaning of life. First, I need to consider different perspectives. The user might be looking for a\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to get started with LLMs. Let me break this down.\n",
      "\n",
      "First, I need to explain the\n",
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the meaning of life. First, I need to consider different perspectives. The user might be looking for a philosophical answer, or perhaps they're interested in the\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to get started with LLMs. Let me break this down.\n",
      "\n",
      "First, I need to explain the basics of LLMs. Maybe start with what\n",
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the meaning of life. First, I need to consider different perspectives. The user might be looking for a philosophical answer, or perhaps they're interested in the meaning of life in a more personal or spiritual sense\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to get started with LLMs. Let me break this down.\n",
      "\n",
      "First, I need to explain the basics of LLMs. Maybe start with what an LLM is. Then, the user might\n",
      "==================================================\n",
      "request 0:\n",
      " <|im_start|>user\n",
      "What is the meaning of life?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the meaning of life. First, I need to consider different perspectives. The user might be looking for a philosophical answer, or perhaps they're interested in the meaning of life in a more personal or spiritual sense.\n",
      "\n",
      "I should mention that the meaning of life can\n",
      "request 1:\n",
      " <|im_start|>user\n",
      "How do I get started with LLMs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking how to get started with LLMs. Let me break this down.\n",
      "\n",
      "First, I need to explain the basics of LLMs. Maybe start with what an LLM is. Then, the user might be confused about the process of getting started with L\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is the meaning of life?\",\n",
    "    \"How do I get started with LLMs?\"\n",
    "]\n",
    "reqs = generate(model, qwen3_tokenizer, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280fbe66-b0cb-4284-a7b1-f7f3f167aca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
